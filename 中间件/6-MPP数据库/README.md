
### MPP数据库(大规模并行处理)


任务并行分散到多个服务器节点。每个节点计算完后汇总在一起

MPP架构。如：Doris、TiDB、Presto

NewSql(既能存储海量数据也能提供事务)如：TiDB


大部分数据库都是2层架构：执行器和存储引擎

#### TIDB

每个节点最大支持 1000

2层架构：
1、TiDBServer(SQL层):负责客户端连接，执行sql解析、优化、生成分布式执行计划
2、PD：元信息管理模块
3、存储节点(TiKV或者TiFlash)

##### 存储

基于RocksDB实现KV存储

使用Raft协议来保证：选举、数据同步(Raft有日志复制同步功能)、多副本高可用

实现了MVCC(多版本并发控制):即通过对key后面添加版本号来实现 key-value->key_Versioin1->value

数据包括：每一行的数据、表所有的索引数据
> 数据存储Key：TableID、RowID(如果有主键使用主键)结合生成(tablePrefix{TableID}_recordPrefixSep{RowID})
>

##### 计算

分布式计算，分发到每个节点计算(更贴近存储层，减少rpc)

##### 应用

TiDB有分区概念，用来分布式存储,只能对一个字段进行分区
有range分区和hash分区。一般采用range分区

与mysql相同，TiDB可以建立多个索引来加速查询


#### RocksDB(一个存储引擎)

Facebook开源的高性能持久化KV存储

常见的 MariaDB、Flink的state、MongoDB 都是基于RocksDB来做存储引擎

##### LSM-Tree

RocksDB快的原因是存储使用了LSM-Tree的数据结构(内存+磁盘。同时数据分层存储)

写入流程：

1、kv写入日志中(磁盘)，用于故障恢复

2、写入内存中的MemTable(一个跳表结构)。写入成功

3、memTable满了后转化成Immutable MemTable。此时有后台线程吧memTable刷到磁盘

4、采用分层合并机制：每一层写满后，触发后台线程往下一层合并。每一层的多个文件和文件中的k是有序的，查找也比较快

删除：

1、标记删除：在内存中使用墓碑删除，在磁盘就插入一条删除记录。在层级合并时真正发挥作用

2、log记录删除操作


#### Doris

向量化、MPP 架构、列式存储引擎。最高可达2W QPS

Doris常用的StarRocks，不支持事务

架构总体分为 FE、BE

> FE保留完整的元数据，提供客户端连接、查询规划
>
> 分为Leader、Follower(参与选举)、Observer(扩展查询能力，不参与选举。选择性部署)
>
> 只有leader有元数据写权限


##### 存储

1、列存储，建立稀疏索引

2、支持把表数据全部缓存在内存中

3、对一个字段range分区、对多个字段hash分桶

4、对应的模型key，会建立排序和稀疏索引(1024列建一个稀疏索引)。索引是36字节的前缀索引

##### 数据模型

Aggregate聚合模型

Unique唯一模型

Duplicate明细模型

##### Rollup表

前2种可以进行聚合操作。明细模型只能进行顺序调整

本质上是对原始表的物化索引


##### 应用

Doris有分区分桶的概念，只能对一个字段进行分区
分桶的目的是减少每个桶文件的大小，一般建议500M~1G

分桶采用hash分桶，所以不要有范围查询的字段(如时间)


##### 性能优化

1、数据模型

2、内存表

3、分区、分桶

4、模型key排序和稀疏索引

5、物化、rollup

6、列式存储的优化(向量化执行，数据压缩)


#### Presto

Facebook开源的MPP Sql引擎。Presto不存储数据，是一个内存计算引擎，分离计算层和存储层

使用Connector SPI实现对各种数据源的访问

采用通用的Master/Slave架构：一个Coordinator多个Worker

> Coodinator负责解析SQL语句，生成执行计划，分发执行任务给Worker节点。还负责整个集群的内存管理
>
> Worker节点负责执行查询任务

##### 执行步骤

1、语法分析：使用Antlr4进行语法分析

2、逻辑计划生成

3、查询优化(剪枝操作、谓词下推、条件下推)

计算引擎Presto从不同方面，减少IO：数据源加载、shuffle

> 谓词下推：把筛选条件下推到叶子结点，交给数据源执行。减少数据源与计算引擎的IO
>
> 节点间并行：如 同分区可以放到一个节点执行
>
> 节点内并行：如 多线程构建hashJoin的hash表

4、查询调度

分为stage(阶段，即每个计划的执行阶段)调度和task调度

stage之间顺序执行，每个stage里面多个task


##### 内存管理

Presto每个work有3个线程池(system、正常、大查询)

1、计算Query需要消耗的内存，再根据所有work节点内存情况进行拆分(充分利用并发度)

2、当查询消耗内存超过集群最大内存，直接kill掉查询

3、多个查询和超过集群内存，就排队


##### 快的原因

1、完全基于内存。比 Hive中间数据写磁盘，Spark 溢出的数据写盘

2、常驻Work进程和线程池。

#### Spark

##### 内存管理

每个计算节点的Executor进程中进行内存管理

只关心吧任务分发到节点

> 也是基于内存计算，如果内存不够就保存到磁盘


##### 与Presto比较

1、太大数据量的使用Spark

2、计算特别复杂的Spark合适


#### 列式存储

1、有助于减少查询数据量

2、可以根据格式更好的压缩文件

3、向量化数据处理

##### 向量化

CPU执行指令时，普通的是一条数据执行sum操作，得到结果

向量化，使用一组数据 执行同一个操作执行，得到一组结果




